## K近邻算法

### K近邻算法简介

- 定义
  - 通过你的“邻居”来判断你属于哪个类别
- 如何计算你到你邻居的距离
  - 一般都是使用欧氏距离

### K近邻算法API 

- sklearn
  - 优势
    - 文档多，且规范
    - 包含的算法多
    - 实现起来容易
- sklearn 中包含的内容
  - 分类、聚类、回归
  - 特征工程
  - 模型选择、调优
- API
  - sklearn.neighbors.KNeighborsClassifier()
    - n_neighbors  -- 选定的参考值
- 机器学习中实现的过程
  - 实例化一个估计器
  - 使用fit 方法进行训练

### 距离度量

- 欧式距离
  - 通过距离的平方值，进行计算
- 曼哈顿距离
  - 通过距离的绝对值，进行计算
- 切比雪夫距离
  - 维度的最大值，进行计算
- 闵可夫斯基距离
  - 当 p=1 时，就是曼哈顿距离
  - 当 p=2 时，就是欧式距离
  - 当 p无穷大时，就是切比雪夫距离

> 小结：前面的四个距离公式都是把单位相同看待，所以计算过程不是很科学

- 标准化欧氏距离
  - 在计算过程中添加了标准差，对量纲数据进行处理
- 余弦距离
  - 通过 cos 思想完成
- 汉明距离
  - 一个字符串到另一个字符串需要变换几个字母，进行统计
- 杰卡德距离
  - 通过交并集，进行统计
- 马氏距离
  - 通过样本分布，进行计算

### K值选择

- 过小
  - 容易受到异常点的影响
  - 过拟合
- 过大
  - 容易受到样本均衡问题的影响
  - 欠拟合
- 拓展
  - 近似误差  --  过拟合
  - 估计误差好， 才是真的好
