## K近邻算法

### K近邻算法简介

- 定义
  - 通过你的“邻居”来判断你属于哪个类别
- 如何计算你到你邻居的距离
  - 一般都是使用欧氏距离

### K近邻算法API

- sklearn
  - 优势
    - 文档多，且规范
    - 包含的算法多
    - 实现起来容易
- sklearn 中包含的内容
  - 分类、聚类、回归
  - 特征工程
  - 模型选择、调优
- API
  - sklearn.neighbors.KNeighborsClassifier()
    - n_neighbors  -- 选定的参考值
- 机器学习中实现的过程
  - 实例化一个估计器
  - 使用fit 方法进行训练

### 距离度量

- 欧式距离
  - 通过距离的平方值，进行计算
- 曼哈顿距离
  - 通过距离的绝对值，进行计算
- 切比雪夫距离
  - 维度的最大值，进行计算
- 闵可夫斯基距离
  - 当 p=1 时，就是曼哈顿距离
  - 当 p=2 时，就是欧式距离
  - 当 p无穷大时，就是切比雪夫距离

> 小结：前面的四个距离公式都是把单位相同看待，所以计算过程不是很科学

- 标准化欧氏距离
  - 在计算过程中添加了标准差，对量纲数据进行处理
- 余弦距离
  - 通过 cos 思想完成
- 汉明距离
  - 一个字符串到另一个字符串需要变换几个字母，进行统计
- 杰卡德距离
  - 通过交并集，进行统计
- 马氏距离
  - 通过样本分布，进行计算

### K值选择

- 过小
  - 容易受到异常点的影响
  - 过拟合
- 过大
  - 容易受到样本均衡问题的影响
  - 欠拟合
- 拓展
  - 近似误差  --  过拟合
  - 估计误差好， 才是真的好

### K近邻算法总结

- 优点
  - 简单有效
  - 重新训练代价低
  - 适合类域交叉样本
  - 适合大样本自动分类
- 缺点
  - 惰性学习
  - 类别评分不是很规范
  - 输出，可解释性不强
  - 对不均衡样本不擅长
    - 样本不均衡  -- 收集到的数据，每个类别占比严重失衡
  - 计算量较大

### 交叉验证 和 网格搜索

- 交叉验证
  - 定义 -- 将拿到的训练数据，分为训练集和验证集
    - 几折交叉验证
  - 分割方式
    - 训练集  -- 训练+验证
    - 测试集  -- 测试
  - 为什么需要交叉验证
    - 为了让被评估的模型，更加的准确可信
    - 交叉验证不能提高模型的准确率
- 网格搜索
  - 超参数
    - sklearn中，需要手动指定的参数，叫做超参数
  - 网格搜索 -- 就是把超参数的值，通过字典的形式传递进去，然后选择最优值
- API
  - sklearn.model_selection.GridSearchCV
    - estimator  --  选择哪个训练模型
    - param_grid  -- 需要传递的超参数
    - cv  -- 几折交叉验证
